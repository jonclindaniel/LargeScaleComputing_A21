{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7680a36",
   "metadata": {},
   "source": [
    "## Querying Streaming Spark DataFrames in an EMR Notebook\n",
    "\n",
    "In this notebook, we will read data from a modified version of the Kinesis stream from last week into a Spark streaming DataFrame. Once we've loaded our streaming DataFrame, we'll perform a simple query on it and write the results of our query to S3 for further analysis.\n",
    "\n",
    "We've provided code for starting a producer and a kinesis stream in a script called `start_stream.py` (in our class GitHub repository). This script sends streaming tweet-like JSON data into our `test_stream` Kinesis stream in the form of `{\"username\": ..., \"age\": ..., \"num_followers\": ..., \"tweet\": ...}`. If you're following along with the code in this notebook, you should run this script locally to send streaming data into the stream (which you can then work with in this notebook). Just be sure to delete your kinesis stream when you're finished with this notebook (we provided a short script that will do this for you, called `delete_stream.py`)\n",
    "\n",
    "First, let's add the [Spark Structured Streaming package](https://spark.apache.org/docs/2.4.7/structured-streaming-programming-guide.html) to our session configuration (we'll specifically add a version that makes it possible to interact with Kinesis streams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7741bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure -f\n",
    "{ \"conf\": {\"spark.jars.packages\": \"com.qubole.spark/spark-sql-kinesis_2.11/1.1.3-spark_2.4\" }}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b72c07",
   "metadata": {},
   "source": [
    "Then, we're ready to start reading from our Kinesis stream. For this demonstration, we'll start with the latest data in the stream, but we could get more granular if we would like to do so as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad98ca4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, json_tuple\n",
    "import time\n",
    "\n",
    "stream_df = spark.readStream \\\n",
    "                 .format('kinesis') \\\n",
    "                 .option('streamName', 'test_stream') \\\n",
    "                 .option('endpointUrl', 'https://kinesis.us-east-1.amazonaws.com')\\\n",
    "                 .option('region', 'us-east-1') \\\n",
    "                 .option('startingposition', 'LATEST')\\\n",
    "                 .load()\n",
    "\n",
    "if stream_df.isStreaming:\n",
    "    print('======================')\n",
    "    print('DataFrame is streaming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e411842c",
   "metadata": {},
   "source": [
    "Now that we have our streaming DataFrame ready, let's use Spark SQL `select` and `where` methods to query our streaming DataFrame. We'll then write this data out to one of an S3 bucket (you'll need to specify your own and then append it with `/data` and `/checkpoints` directories to follow along). Individual CSVs will be produced for each set of data that is processed in a micro-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a287bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ea99662abf4251b7d42d269205193a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# start process of querying streaming data\n",
    "query = stream_df.selectExpr('CAST(data AS STRING)', 'CAST(approximateArrivalTimestamp as TIMESTAMP)') \\\n",
    "    .select('approximateArrivalTimestamp', \n",
    "            json_tuple(col('data'), 'username', 'age', 'num_followers', 'tweet'\n",
    "                      ).alias('username', 'age', 'num_followers', 'tweet')) \\\n",
    "    .select('approximateArrivalTimestamp', 'username', 'age') \\\n",
    "    .where('age > 35') \\\n",
    "    .writeStream \\\n",
    "    .queryName('counts') \\\n",
    "    .outputMode('append') \\\n",
    "    .format('csv') \\\n",
    "    .option('path', 's3://mrjob-634d5d805a7e423b/data') \\\n",
    "    .option('checkpointLocation','s3://mrjob-634d5d805a7e423b/checkpoints') \\\n",
    "    .start()\n",
    "\n",
    "# let streaming query run for 15 seconds (and continue sending results to CSV in S3), then stop it\n",
    "time.sleep(15)\n",
    "\n",
    "# Stop query; look at results of micro-batch queries in S3 bucket in `/data` directory\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16503ad9",
   "metadata": {},
   "source": [
    "Cool! If we take a look at one of our resulting CSVs over in our S3 bucket (see head below), we can see that it produces the expected results (a selection of columns from the streaming data that is filtered by age). This is a great way to quickly process streaming data!\n",
    "\n",
    "```\n",
    "2021-11-06T22:02:57.862Z,Deangelo,69\n",
    "2021-11-06T22:02:58.007Z,Arron,80\n",
    "2021-11-06T22:02:58.044Z,Compton,88\n",
    "2021-11-06T22:02:58.081Z,Mabel,95\n",
    "2021-11-06T22:02:58.117Z,John,62\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
